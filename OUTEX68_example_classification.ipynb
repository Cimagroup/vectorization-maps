{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea1aac8",
   "metadata": {},
   "source": [
    "# Classifying textures in outex68 using TDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3dd4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_heatmap\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import gudhi as gd\n",
    "import gudhi.representations\n",
    "import vectorization as ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdac8f",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258516bc",
   "metadata": {},
   "source": [
    "We will try to classify outex-68. First, we load the .bmp images as matrixes. It will take some seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d07c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Outex-TC-00024/data/images'\n",
    "images_names = os.listdir(folder)\n",
    "images_names = list(filter(lambda x : x[0]!='.', images_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ed921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_matrixes = np.array(list(map(lambda x : io.imread(folder+'/'+x), images_names)), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf4e61",
   "metadata": {},
   "source": [
    "Then, we pick the names of the training and test images and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8ed23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = pd.read_csv(\"/home/manu/vectorisation-maps/Outex-TC-00024/data/000/train.txt\", sep=\" \", usecols=[0]).to_numpy().flatten().tolist()\n",
    "train_labels = pd.read_csv(\"/home/manu/vectorisation-maps/Outex-TC-00024/data/000/train.txt\", sep=\" \", usecols=[1]).to_numpy().flatten().tolist()\n",
    "test_names = pd.read_csv(\"/home/manu/vectorisation-maps/Outex-TC-00024/data/000/test.txt\", sep=\" \", usecols=[0]).to_numpy().flatten().tolist()\n",
    "test_labels = pd.read_csv(\"/home/manu/vectorisation-maps/Outex-TC-00024/data/000/test.txt\", sep=\" \", usecols=[1]).to_numpy().flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492de645",
   "metadata": {},
   "source": [
    "Using the file names, we will load the train and test images. Note that they will keep the same order than the labels. We will also use the images with the greyscale inverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a70fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = list(map(lambda x : images_names.index(x), train_names))\n",
    "test_indexes = list(map(lambda x : images_names.index(x), test_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ffe94",
   "metadata": {},
   "source": [
    "We are using only 10 classes from the original dataset. In order to perform the experiment with the 68 classes, comment the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278fcca",
   "metadata": {},
   "source": [
    "from numpy.random import choice\n",
    "labels = np.array(list(set(test_labels)))\n",
    "labels = choice(labels, size=(10), replace = False)\n",
    "    \n",
    "train_indexes = [train_indexes[i] for i in range(len(train_indexes)) if train_labels[i] in labels]\n",
    "train_labels = [i for i in train_labels if i in labels]\n",
    "test_indexes = [test_indexes[i] for i in range(len(test_indexes)) if test_labels[i] in labels]\n",
    "test_labels = [i for i in test_labels if i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75510587",
   "metadata": {},
   "source": [
    "Finally, we load the images and their inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8e2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_gudhi = np.array(list(map(lambda x : x.reshape(128*128,1), images_matrixes)))\n",
    "train_gudhi =  images_gudhi[train_indexes]\n",
    "test_gudhi = images_gudhi[test_indexes]\n",
    "\n",
    "train_gudhi_opp =  255-train_gudhi\n",
    "test_gudhi_opp = 255-test_gudhi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79dc2d",
   "metadata": {},
   "source": [
    "## Calculating the persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f23b8",
   "metadata": {},
   "source": [
    "We use the filter function for cubical complexes to obtain the persistence diagrams. We obtain 4 diagrams from each image: dimension 0 and 1 of the original image and the inverted one. It takes around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbeccd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_filtration = lambda image : gd.CubicalComplex(dimensions = [128,128], top_dimensional_cells=image)\n",
    "calculate_pd = lambda image : cub_filtration(image).persistence()\n",
    "\n",
    "train_pds = list(map(calculate_pd, train_gudhi))\n",
    "test_pds = list(map(calculate_pd, test_gudhi))\n",
    "train_pds_opp = list(map(calculate_pd, train_gudhi_opp))\n",
    "test_pds_opp = list(map(calculate_pd, test_gudhi_opp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f0125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The representation module does not deal well with infinity, so we change it by 256.\n",
    "infty_proj = lambda x : 256 if ~np.isfinite(x) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95964be",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_of_dim_0 = lambda pd : np.array([[x[1][0], infty_proj(x[1][1])]  for x in pd if x[0]==0])\n",
    "train_pds_0 = list(map(intervals_of_dim_0, train_pds))\n",
    "test_pds_0 = list(map(intervals_of_dim_0, test_pds))\n",
    "train_pds_opp_0 = list(map(intervals_of_dim_0, train_pds_opp))\n",
    "test_pds_opp_0 = list(map(intervals_of_dim_0, test_pds_opp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea1869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_of_dim_1 = lambda pd : np.array([[x[1][0], infty_proj(x[1][1])]  for x in pd if x[0]==1])\n",
    "train_pds_1 = list(map(intervals_of_dim_1, train_pds))\n",
    "test_pds_1 = list(map(intervals_of_dim_1, test_pds))\n",
    "train_pds_opp_1 = list(map(intervals_of_dim_1, train_pds_opp))\n",
    "test_pds_opp_1 = list(map(intervals_of_dim_1, test_pds_opp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb43484",
   "metadata": {},
   "source": [
    "## Classification with the Betti Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885305cb",
   "metadata": {},
   "source": [
    "We perform a classification task using the betti curve and Random Forest. We concatenate the four betti curves obtained from each image, and then feed the classifier with them. It takes 1 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 100\n",
    "train_Btt_0 = [ex.GetBettiCurveFeature(pd, res) for pd in train_pds_0]\n",
    "train_Btt_1 = [ex.GetBettiCurveFeature(pd, res) for pd in train_pds_1]\n",
    "test_Btt_0 = [ex.GetBettiCurveFeature(pd, res) for pd in test_pds_0]\n",
    "test_Btt_1 = [ex.GetBettiCurveFeature(pd, res) for pd in test_pds_1]\n",
    "train_Btt_opp_0 = [ex.GetBettiCurveFeature(pd, res) for pd in train_pds_opp_0]\n",
    "train_Btt_opp_1 = [ex.GetBettiCurveFeature(pd, res) for pd in train_pds_opp_1]\n",
    "test_Btt_opp_0 = [ex.GetBettiCurveFeature(pd, res) for pd in test_pds_opp_0]\n",
    "test_Btt_opp_1 = [ex.GetBettiCurveFeature(pd, res) for pd in test_pds_opp_1]\n",
    "train_Btt = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_Btt_0, train_Btt_1, train_Btt_opp_0, train_Btt_opp_1)]\n",
    "test_Btt = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_Btt_0, test_Btt_1, test_Btt_opp_0, test_Btt_opp_1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9efe27",
   "metadata": {},
   "source": [
    "Example of how the curves look like after the concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_Btt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97652008",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_Btt, train_labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Btt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad402a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(train_Btt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c146f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dda39b",
   "metadata": {},
   "source": [
    "In this example, the accuracy is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94911fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_Btt, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_Btt, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a64542",
   "metadata": {},
   "source": [
    "## Classification with PersStats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59b6e8",
   "metadata": {},
   "source": [
    "We use here persistent stats. It takes less than one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa58ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sta_0 = [ex.GetPersStats(pd) for pd in train_pds_0]\n",
    "train_Sta_1 = [ex.GetPersStats(pd) for pd in train_pds_1]\n",
    "test_Sta_0 = [ex.GetPersStats(pd) for pd in test_pds_0]\n",
    "test_Sta_1 = [ex.GetPersStats(pd) for pd in test_pds_1]\n",
    "train_Sta_opp_0 = [ex.GetPersStats(pd) for pd in train_pds_opp_0]\n",
    "train_Sta_opp_1 = [ex.GetPersStats(pd) for pd in train_pds_opp_1]\n",
    "test_Sta_opp_0 = [ex.GetPersStats(pd) for pd in test_pds_opp_0]\n",
    "test_Sta_opp_1 = [ex.GetPersStats(pd) for pd in test_pds_opp_1]\n",
    "train_Sta = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_Sta_0, train_Sta_1, train_Sta_opp_0, train_Sta_opp_1)]\n",
    "test_Sta = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_Sta_0, test_Sta_1, test_Sta_opp_0, test_Sta_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbd9ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000)\n",
    "classifier = classifier.fit(train_Sta, train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b83a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 1.0\n",
      "Test accuracy  = 0.5691176470588235\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_Sta, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_Sta, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd954b",
   "metadata": {},
   "source": [
    "## Classification with Persistence Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38096a72",
   "metadata": {},
   "source": [
    "We use here persistent stats. It takes less than five minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd576cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [6,6]\n",
    "\n",
    "train_PI_0 = [ex.GetPersImageFeature(pd,res) for pd in train_pds_0]\n",
    "train_PI_1 = [ex.GetPersImageFeature(pd,res) for pd in train_pds_1]\n",
    "test_PI_0 = [ex.GetPersImageFeature(pd,res) for pd in test_pds_0]\n",
    "test_PI_1 = [ex.GetPersImageFeature(pd,res) for pd in test_pds_1]\n",
    "train_PI_opp_0 = [ex.GetPersImageFeature(pd,res) for pd in train_pds_opp_0]\n",
    "train_PI_opp_1 = [ex.GetPersImageFeature(pd,res) for pd in train_pds_opp_1]\n",
    "test_PI_opp_0 = [ex.GetPersImageFeature(pd,res) for pd in test_pds_opp_0]\n",
    "test_PI_opp_1 = [ex.GetPersImageFeature(pd,res) for pd in test_pds_opp_1]\n",
    "train_PI = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_PI_0, train_PI_1, train_PI_opp_0, train_PI_opp_1)]\n",
    "test_PI = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_PI_0, test_PI_1, test_PI_opp_0, test_PI_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd753980",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_PI, train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_PI, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_PI, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e216e53",
   "metadata": {},
   "source": [
    "## Classification with Entropy Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43528a73",
   "metadata": {},
   "source": [
    "We use here persistent stats. It takes less than five minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca311f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_E_0 = [ex.GetPersEntropyFeature(pd) for pd in train_pds_0]\n",
    "train_E_1 = [ex.GetPersEntropyFeature(pd) for pd in train_pds_1]\n",
    "test_E_0 = [ex.GetPersEntropyFeature(pd) for pd in test_pds_0]\n",
    "test_E_1 = [ex.GetPersEntropyFeature(pd) for pd in test_pds_1]\n",
    "train_E_opp_0 = [ex.GetPersEntropyFeature(pd) for pd in train_pds_opp_0]\n",
    "train_E_opp_1 = [ex.GetPersEntropyFeature(pd) for pd in train_pds_opp_1]\n",
    "test_E_opp_0 = [ex.GetPersEntropyFeature(pd) for pd in test_pds_opp_0]\n",
    "test_E_opp_1 = [ex.GetPersEntropyFeature(pd) for pd in test_pds_opp_1]\n",
    "train_E = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_E_0, train_E_1, train_E_opp_0, train_E_opp_1)]\n",
    "test_E = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_E_0, test_E_1, test_E_opp_0, test_E_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_E, train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d088d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_E, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_E, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654e10f",
   "metadata": {},
   "source": [
    "## Classification with Life Span curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa468557",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Life_0 = [ex.GetPersLifespanFeature(pd) for pd in train_pds_0]\n",
    "train_Life_1 = [ex.GetPersLifespanFeature(pd) for pd in train_pds_1]\n",
    "test_Life_0 = [ex.GetPersLifespanFeature(pd) for pd in test_pds_0]\n",
    "test_Life_1 = [ex.GetPersLifespanFeature(pd) for pd in test_pds_1]\n",
    "train_Life_opp_0 = [ex.GetPersLifespanFeature(pd) for pd in train_pds_opp_0]\n",
    "train_Life_opp_1 = [ex.GetPersLifespanFeature(pd) for pd in train_pds_opp_1]\n",
    "test_Life_opp_0 = [ex.GetPersLifespanFeature(pd) for pd in test_pds_opp_0]\n",
    "test_Life_opp_1 = [ex.GetPersLifespanFeature(pd) for pd in test_pds_opp_1]\n",
    "train_Life = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_Life_0, train_Life_1, train_Life_opp_0, train_Life_opp_1)]\n",
    "test_Life = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_Life_0, test_Life_1, test_Life_opp_0, test_Life_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_Life, train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb45f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_Life, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_Life, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6d427",
   "metadata": {},
   "source": [
    "## Classification with Tropical Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ce721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_T_0 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in train_pds_0]\n",
    "train_T_1 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in train_pds_1]\n",
    "test_T_0 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in test_pds_0]\n",
    "test_T_1 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in test_pds_1]\n",
    "train_T_opp_0 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in train_pds_opp_0]\n",
    "train_T_opp_1 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in train_pds_opp_1]\n",
    "test_T_opp_0 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in test_pds_opp_0]\n",
    "test_T_opp_1 = [ex.GetPersTropicalCoordinatesFeature(pd) for pd in test_pds_opp_1]\n",
    "train_T = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_T_0, train_T_1, train_T_opp_0, train_T_opp_1)]\n",
    "test_T = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_T_0, test_T_1, test_T_opp_0, test_T_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e46894",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_T, train_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded734cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_T, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_T, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc084f7",
   "metadata": {},
   "source": [
    "## Classification with Atol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "quantiser = KMeans(n_clusters=2, random_state=202006)\n",
    "\n",
    "train_A_0 = [ex.GetAtolFeature(pd, quantiser) for pd in train_pds_0]\n",
    "train_A_1 = [ex.GetAtolFeature(pd, quantiser) for pd in train_pds_1]\n",
    "test_A_0 = [ex.GetAtolFeature(pd, quantiser) for pd in test_pds_0]\n",
    "test_A_1 = [ex.GetAtolFeature(pd, quantiser) for pd in test_pds_1]\n",
    "train_A_opp_0 = [ex.GetAtolFeature(pd, quantiser) for pd in train_pds_opp_0]\n",
    "train_A_opp_1 = [ex.GetAtolFeature(pd, quantiser) for pd in train_pds_opp_1]\n",
    "test_A_opp_0 = [ex.GetAtolFeature(pd, quantiser) for pd in test_pds_opp_0]\n",
    "test_A_opp_1 = [ex.GetAtolFeature(pd, quantiser) for pd in test_pds_opp_1]\n",
    "train_A = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_A_0, train_A_1, train_A_opp_0, train_A_opp_1)]\n",
    "test_A = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_A_0, test_A_1, test_A_opp_0, test_A_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_A, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_A, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_A, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf9318",
   "metadata": {},
   "source": [
    " ## Classification with Persistence Landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Land_0 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in train_pds_0]\n",
    "train_Land_1 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in train_pds_1]\n",
    "test_Land_0 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in test_pds_0]\n",
    "test_Land_1 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in test_pds_1]\n",
    "train_Land_opp_0 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in train_pds_opp_0]\n",
    "train_Land_opp_1 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in train_pds_opp_1]\n",
    "test_Land_opp_0 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in test_pds_opp_0]\n",
    "test_Land_opp_1 = [ex.GetPersLandscapeFeature(pd, num=20) for pd in test_pds_opp_1]\n",
    "train_Land = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_Land_0, train_Land_1, train_Land_opp_0, train_Land_opp_1)]\n",
    "test_Land = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_Land_0, test_Land_1, test_Land_opp_0, test_Land_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_Land, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_Land, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_Land, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6561a",
   "metadata": {},
   "source": [
    " ## Classification with Persistence Silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac98f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sil_0 = [ex.GetPersSilhouetteFeature(pd) for pd in train_pds_0]\n",
    "train_Sil_1 = [ex.GetPersSilhouetteFeature(pd) for pd in train_pds_1]\n",
    "test_Sil_0 = [ex.GetPersSilhouetteFeature(pd) for pd in test_pds_0]\n",
    "test_Sil_1 = [ex.GetPersSilhouetteFeature(pd) for pd in test_pds_1]\n",
    "train_Sil_opp_0 = [ex.GetPersSilhouetteFeature(pd) for pd in train_pds_opp_0]\n",
    "train_Sil_opp_1 = [ex.GetPersSilhouetteFeature(pd) for pd in train_pds_opp_1]\n",
    "test_Sil_opp_0 = [ex.GetPersSilhouetteFeature(pd) for pd in test_pds_opp_0]\n",
    "test_Sil_opp_1 = [ex.GetPersSilhouetteFeature(pd) for pd in test_pds_opp_1]\n",
    "train_Sil = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_Sil_0, train_Sil_1, train_Sil_opp_0, train_Sil_opp_1)]\n",
    "test_Sil = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_Sil_0, test_Sil_1, test_Sil_opp_0, test_Sil_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_Sil, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_Sil, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_Sil, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a07064",
   "metadata": {},
   "source": [
    " ## Classification with Carlsson Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f07487",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CC_0 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in train_pds_0]\n",
    "train_CC_1 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in train_pds_1]\n",
    "test_CC_0 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in test_pds_0]\n",
    "test_CC_1 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in test_pds_1]\n",
    "train_CC_opp_0 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in train_pds_opp_0]\n",
    "train_CC_opp_1 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in train_pds_opp_1]\n",
    "test_CC_opp_0 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in test_pds_opp_0]\n",
    "test_CC_opp_1 = [ex.GetCarlssonCoordinatesFeature(pd) for pd in test_pds_opp_1]\n",
    "train_CC = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_CC_0, train_CC_1, train_CC_opp_0, train_CC_opp_1)]\n",
    "test_CC = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_CC_0, test_CC_1, test_CC_opp_0, test_CC_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35512539",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_CC, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_CC, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_CC, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb89e6",
   "metadata": {},
   "source": [
    " ## Classification with Topological Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_TV_0 = [ex.GetTopologicalVectorFeature(pd) for pd in train_pds_0]\n",
    "train_TV_1 = [ex.GetTopologicalVectorFeature(pd) for pd in train_pds_1]\n",
    "test_TV_0 = [ex.GetTopologicalVectorFeature(pd) for pd in test_pds_0]\n",
    "test_TV_1 = [ex.GetTopologicalVectorFeature(pd) for pd in test_pds_1]\n",
    "train_TV_opp_0 = [ex.GetTopologicalVectorFeature(pd) for pd in train_pds_opp_0]\n",
    "train_TV_opp_1 = [ex.GetTopologicalVectorFeature(pd) for pd in train_pds_opp_1]\n",
    "test_TV_opp_0 = [ex.GetTopologicalVectorFeature(pd) for pd in test_pds_opp_0]\n",
    "test_TV_opp_1 = [ex.GetTopologicalVectorFeature(pd) for pd in test_pds_opp_1]\n",
    "train_TV = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_TV_0, train_TV_1, train_TV_opp_0, train_TV_opp_1)]\n",
    "test_TV = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_TV_0, test_TV_1, test_TV_opp_0, test_TV_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_TV, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_TV, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_TV, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddd07c",
   "metadata": {},
   "source": [
    " ## Classification with Complex Polynomials (type = R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff35ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CPR_0 = [ex.GetComplexPolynomialFeature(pd) for pd in train_pds_0]\n",
    "train_CPR_1 = [ex.GetComplexPolynomialFeature(pd) for pd in train_pds_1]\n",
    "test_CPR_0 = [ex.GetComplexPolynomialFeature(pd) for pd in test_pds_0]\n",
    "test_CPR_1 = [ex.GetComplexPolynomialFeature(pd) for pd in test_pds_1]\n",
    "train_CPR_opp_0 = [ex.GetComplexPolynomialFeature(pd) for pd in train_pds_opp_0]\n",
    "train_CPR_opp_1 = [ex.GetComplexPolynomialFeature(pd) for pd in train_pds_opp_1]\n",
    "test_CPR_opp_0 = [ex.GetComplexPolynomialFeature(pd) for pd in test_pds_opp_0]\n",
    "test_CPR_opp_1 = [ex.GetComplexPolynomialFeature(pd) for pd in test_pds_opp_1]\n",
    "train_CPR = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_CPR_0, train_CPR_1, train_CPR_opp_0, train_CPR_opp_1)]\n",
    "test_CPR = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_CPR_0, test_CPR_1, test_CPR_opp_0, test_CPR_opp_1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f3a7e",
   "metadata": {},
   "source": [
    "The sk-learn classifiers requires float32, but GetComplexPolynomialFeature can give large float64 values. One solution is to study what is the maximum value of the experiment and divide all values by a fixed large $10^n$, to make them smaller than $10^{38}$. Then, all values can be transform to float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69293f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_index(x):\n",
    "    if ((len(str(x))>4)and(str(x)[-4] =='e')):\n",
    "        y = int(str(x)[-3:])  \n",
    "    else:\n",
    "        y = len(str(x))\n",
    "    return y\n",
    "y = max([max([e_index(c) for c in cp]) for cp in train_CPR] +\n",
    "       [max([e_index(c) for c in cp]) for cp in test_CPR])\n",
    "n = max(0, y-38+1)\n",
    "print('The maximum number has the following size:')\n",
    "print('10 power ', y)\n",
    "print('We shall divide by 10 power', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CPR = [[np.float32(x/10**n) for x in cp] for cp in train_CPR]\n",
    "test_CPR = [[np.float32(x/10**n) for x in cp] for cp in test_CPR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa96eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_CPR, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_CPR, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_CPR, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b2181",
   "metadata": {},
   "source": [
    " ## Classification with Complex Polynomials (type = S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7e9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_CPS_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in train_pds_0]\n",
    "train_CPS_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in train_pds_1]\n",
    "test_CPS_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in test_pds_0]\n",
    "test_CPS_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in test_pds_1]\n",
    "train_CPS_opp_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in train_pds_opp_0]\n",
    "train_CPS_opp_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in train_pds_opp_1]\n",
    "test_CPS_opp_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in test_pds_opp_0]\n",
    "test_CPS_opp_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='S') for pd in test_pds_opp_1]\n",
    "train_CPS = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_CPS_0, train_CPS_1, train_CPS_opp_0, train_CPS_opp_1)]\n",
    "test_CPS = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_CPS_0, test_CPS_1, test_CPS_opp_0, test_CPS_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_index(x):\n",
    "    if ((len(str(x))>4)and(str(x)[-4] =='e')):\n",
    "        y = int(str(x)[-3:])  \n",
    "    else:\n",
    "        y = len(str(x))\n",
    "    return y\n",
    "y = max([max([e_index(c) for c in cp]) for cp in train_CPS] +\n",
    "       [max([e_index(c) for c in cp]) for cp in test_CPS])\n",
    "n = max(0, y-38+1)\n",
    "print('The maximum number has the following size:')\n",
    "print('10 power ', y)\n",
    "print('We shall divide by 10 power', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CPS = [[np.float32(x/10**n) for x in cp] for cp in train_CPS]\n",
    "test_CPS = [[np.float32(x/10**n) for x in cp] for cp in test_CPS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_CPS, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_CPS, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_CPS, test_labels))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e4d6a",
   "metadata": {},
   "source": [
    " ## Classification with Complex Polynomials (type = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a92a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CPT_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in train_pds_0]\n",
    "train_CPT_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in train_pds_1]\n",
    "test_CPT_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in test_pds_0]\n",
    "test_CPT_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in test_pds_1]\n",
    "train_CPT_opp_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in train_pds_opp_0]\n",
    "train_CPT_opp_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in train_pds_opp_1]\n",
    "test_CPT_opp_0 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in test_pds_opp_0]\n",
    "test_CPT_opp_1 = [ex.GetComplexPolynomialFeature(pd, pol_type='T') for pd in test_pds_opp_1]\n",
    "train_CPT = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(train_CPT_0, train_CPT_1, train_CPT_opp_0, train_CPT_opp_1)]\n",
    "test_CPT = [np.concatenate([dgm0,dgm1,dgm2,dgm3]) for dgm0,dgm1,dgm2,dgm3 in zip(test_CPT_0, test_CPT_1, test_CPT_opp_0, test_CPT_opp_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_index(x):\n",
    "    if ((len(str(x))>4)and(str(x)[-4] =='e')):\n",
    "        y = int(str(x)[-3:])  \n",
    "    else:\n",
    "        y = len(str(x))\n",
    "    return y\n",
    "y = max([max([e_index(c) for c in cp]) for cp in train_CPT] +\n",
    "       [max([e_index(c) for c in cp]) for cp in test_CPT])\n",
    "n = max(0, y-38+1)\n",
    "print('The maximum number has the following size:')\n",
    "print('10 power ', y)\n",
    "print('We shall divide by 10 power', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CPT = [[np.float32(x/10**n) for x in cp] for cp in train_CPT]\n",
    "test_CPT = [[np.float32(x/10**n) for x in cp] for cp in test_CPT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = classifier.fit(train_CPT, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(classifier.score(train_CPT, train_labels)))\n",
    "print(\"Test accuracy  = \" + str(classifier.score(test_CPT, test_labels))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
